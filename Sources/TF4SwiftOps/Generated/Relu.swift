// ---------------------------------------------------------------------------
//  AUTO-GENERATED FILE. DO NOT EDIT.
//  Generated by TF4SwiftOpGen from ops.pbtxt (op: Relu)
// ---------------------------------------------------------------------------

import TF4SwiftCore

public extension Ops {
    /// Generated wrapper for TensorFlow Relu.
    func relu<T: TensorFlowFloatingPoint>(_ x: Tensor<T>, device: String? = nil) throws -> Tensor<T> {
        let outs = try build("Relu")
            .device(device)
            .addInput(x)
            .attr("T", dtype: T.tfDataType)
            .execute(outputs: 1)
        return Tensor<T>.fromOwnedHandle(outs[0])
    }
}

import TF4SwiftCore

public extension _Raw {
    /// Non-throwing wrapper for TensorFlow Relu.
    static func relu<T: TensorFlowFloatingPoint>(features x: Tensor<T>, device: String? = nil) -> Tensor<T> {
        let ctx = _Runtime.defaultContext
        let outs = try! Ops(ctx).build("Relu")
            .device(device)
            .addInput(x)
            .attr("T", dtype: T.tfDataType)
            .execute(outputs: 1)
        return Tensor<T>.fromOwnedHandle(outs[0])
    }
}

// ---------------------------------------------------------------------------
//  AUTO-GENERATED FILE. DO NOT EDIT.
//  Generated by TF4SwiftOpGen from ops.pbtxt (op: ReluGrad)
// ---------------------------------------------------------------------------

import TF4SwiftCore

public extension Ops {
    /// Generated wrapper for TensorFlow ReluGrad.
    func reluGrad<T: TensorFlowFloatingPoint>(_ x: Tensor<T>, _ y: Tensor<T>, device: String? = nil) throws -> Tensor<T> {
        let outs = try build("ReluGrad")
            .device(device)
            .addInput(x)
            .addInput(y)
            .attr("T", dtype: T.tfDataType)
            .execute(outputs: 1)
        return Tensor<T>.fromOwnedHandle(outs[0])
    }
}

import TF4SwiftCore

public extension _Raw {
    /// Non-throwing wrapper for TensorFlow ReluGrad.
    static func reluGrad<T: TensorFlowFloatingPoint>(gradients: Tensor<T>, features: Tensor<T>, device: String? = nil) -> Tensor<T> {
        let ctx = _Runtime.defaultContext
        let outs = try! Ops(ctx).build("ReluGrad")
            .device(device)
            .addInput(gradients)
            .addInput(features)
            .attr("T", dtype: T.tfDataType)
            .execute(outputs: 1)
        return Tensor<T>.fromOwnedHandle(outs[0])
    }
}

#if canImport(_Differentiation)
import _Differentiation
import TF4SwiftCore

@inlinable
@differentiable(reverse, wrt: x)
public func relu<T: TensorFlowFloatingPoint>(_ x: Tensor<T>) -> Tensor<T> {
    _Raw.relu(features: x)
}

@inlinable
@derivative(of: relu)
public func _vjpRelu<T: TensorFlowFloatingPoint>(
    _ x: Tensor<T>
) -> (
    value: Tensor<T>,
    pullback: (Tensor<T>.TangentVector) -> Tensor<T>.TangentVector
) {
    let y = _Raw.relu(features: x)
    return (y, { v in
        guard let seed = v.base else { return .zero }
        let g = _Raw.reluGrad(gradients: seed, features: x)
        return Tensor<T>.TangentVector(g)
    })
}
#endif